---
title: "Robust Gaussian Processes in Stan"
author: "Michael Betancourt"
date: "October 2017"
output:
  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA)
```

[Part 1: Introduction to Gaussian Processes](../gp_part1/part1.html)
Part 2: Optimizing Gaussian Processes Hyperparameters
[Part 3: Bayesian Inference of Gaussian Processes Hyperparameters](../gp_part3/part3.html)

In [Part 1](../gp_part1/part1.html) of this case study we learned the
basics of Gaussian processes and their implementation in Stan.  We
assumed, however, the knowledge of the correct hyperparameters for
the squared exponential kernel, $\alpha$ and $\rho$, as well as the
measurement variability, $\sigma$, in the Gaussian observation model.
In practice we will not know these hyperparameters a priori but will
instead have to infer them from the observed data themselves.

Unfortunately, inferring Gaussian process hyperparameters is a notoriously
challenging problem.  In this part of the case study we will consider
fitting the hyperparameters with maximum marginal likelihood, a computationally
inexpensive technique for generating point estimates and consider the utility
of those point estimates in the context of Gaussian process regression with
a Gaussian observation model.

# Initial Setup

First things first we set up our local computing environment,

```{r, comment=NA}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
source("gp_utility.R")
```

and then load in both the simulated data and the ground truth,

```{r, comment=NA}
data <- read_rdump('gp.data.R')
true_realization <- read_rdump('gp.truth.R')
```

Finally we recreate the true data generating process that we
are attempting to model,

```{r, comment=NA}
f_data <- list(sigma=true_realization$sigma_true,
               N=length(true_realization$f_total),
               f=true_realization$f_total)
dgp_fit <- stan(file='simu_gauss_dgp.stan', data=f_data, iter=1000, warmup=0,
                chains=1, seed=5838298, refresh=1000, algorithm="Fixed_param")

plot_gp_pred_quantiles(dgp_fit, data, true_realization,
                       "True Data Generating Process Quantiles")
```

# Inferring the Hyperparameters with Maximum Marginal Likelihood

Maximum marginal likelihood optimizes the likelihood of the observed
data conditioned on the Gaussian process hyperparameters with the
realizations of the Gaussian process themselves marginalized out.
Note that this is fundamentally different from constructing a
modal estimator by optimizing the posterior _density_ for the
hyperparameters as the maximum marginal likelihood is a _function_.
Differences arise between the two approaches when transformations
are introduced to bound parameters, for example enforcing positivity
of $\alpha$, $\rho$, and $\sigma$.

To implement maximum marginal likelihood we move the now unknown
hyperparameters to the `parameters` block of our Stan program and then
construct a analytic posterior Gaussian process in the `model` block,

```{r, comment=NA}
writeLines(readLines("opt1.stan"))
```

We then optimize the resulting Stan program to give

```{r, comment=NA}
gp_opt1 <- stan_model(file='opt1.stan')
opt_fit <- optimizing(gp_opt1, data=data, seed=5838298, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

Note the large marginal standard deviation, $\alpha$, the small lenght scale,
$\rho$, and small measurement variability, $\sigma$.  Together these imply
a Gaussian process that supports _very_ wiggly functions.

Given the marginal maximum likelihood estimate of the hyperparameters we
can then simulate from this induced Gaussian process posterior distributions
and its corresponding posterior predictive distributions,

```{r, comment=NA}
pred_data <- list(alpha=alpha, rho=rho, sigma=sigma, N=data$N, x=data$x, y=data$y,
                  N_predict=data$N_predict, x_predict=data$x_predict)
pred_opt_fit <- stan(file='predict_gauss.stan', data=pred_data, iter=1000, warmup=0,
                     chains=1, seed=5838298, refresh=1000, algorithm="Fixed_param")
```

Indeed the wiggly functions favored by this fit seriously overfit to the
observed data,

```{r, comment=NA}
plot_gp_realizations(pred_opt_fit, data, true_realization,
                     "Posterior Realizations")
```

```{r, comment=NA}
plot_gp_quantiles(pred_opt_fit, data, true_realization,
                  "Posterior Quantiles")
```

The overfitting is particularly evident in the posterior predictive
visualizations which demonstrate poor out of sample performance,
especially in the neighborhoods near the observed data.

```{r, comment=NA}
plot_gp_realizations(pred_opt_fit, data, true_realization,
                     "Posterior Predictive Realizations")
```

```{r, comment=NA}
par(mfrow=c(1, 2))

plot_gp_pred_quantiles(pred_opt_fit, data, true_realization,
                       "PP Quantiles")

plot_gp_pred_quantiles(dgp_fit, data, true_realization,
                       "True DGP Quantiles")
```

But how reproducible is this result?  Let's try the fit again, only this
time with a different random number generator seed that will select a
different starting point for the hyperparameters.

```{r, comment=NA}
opt_fit <- optimizing(gp_opt1, data=data, seed=2384853, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

The different starting point has yielded a completely different fit,
which should already provoke unease.  In this case the smaller marginal
standard deviation and larger measurement variability produce a somewhat
better fit, although the manifestation of overfitting is still evident.

```{r, comment=NA}
pred_data <- list(alpha=alpha, rho=rho, sigma=sigma, N=data$N, x=data$x, y=data$y,
                  N_predict=data$N_predict, x_predict=data$x_predict)
pred_opt_fit <- stan(file='predict_gauss.stan', data=pred_data, iter=1000, warmup=0,
                     chains=1, seed=5838298, refresh=1000, algorithm="Fixed_param")
```

```{r, comment=NA}
plot_gp_realizations(pred_opt_fit, data, true_realization,
                     "Posterior Realizations")
```

```{r, comment=NA}
plot_gp_quantiles(pred_opt_fit, data, true_realization,
                  "Posterior Quantiles")
```

```{r, comment=NA}
plot_gp_realizations(pred_opt_fit, data, true_realization,
                     "Posterior Predictive Realizations")
```

```{r, comment=NA}
par(mfrow=c(1, 2))

plot_gp_pred_quantiles(pred_opt_fit, data, true_realization,
                       "PP Quantiles")

plot_gp_pred_quantiles(dgp_fit, data, true_realization,
                       "True DGP Quantiles")
```

The results of the hyperparameters with maximum marginal likelihood
are highly sensitive to the initial starting point and of the many
results we get all seem to prone to overfitting.  One potential
means of improving the robustness of these fits is to introduce
_regularization_.  Let's see if that does any better.

# Regularized Maximum Marginal Likelihood

Regularized maximum marginal likelihood introduces a loss functions
to prevent the fit from straying into neighborhoods that are known
to be pathological, such as those that might be prone to overfitting.
Exactly what kind of regularization is needed is unclear at this point
-- indeed we will see in [Part 3](../gp_part3/part3.html) that a
Bayesian workflow is much more adept at identifying what kind of
regularization is required.

Here we will presume a given regularization scheme and leave
its motivation, and further discussion about why unregularized
maximum marginal likelihood is so fragile, to [Part 3](../gp_part3/part3.html).

In order to implement regularization in Stan we add prior densities
to our Stan program that emulate the desired loss functions,

```{r, comment=NA}
writeLines(readLines("opt2.stan"))
```

In particular, the regularization penalizes the small length
scales that we kept getting in the unregularized fits.

```{r, comment=NA}
gp_opt2 <- stan_model(file='opt2.stan')
opt_fit <- optimizing(gp_opt2, data=data, seed=5838298, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

Somewhat more encouraging, running again with a different seed
yields essentially the same result.

```{r, comment=NA}
gp_opt2 <- stan_model(file='opt2.stan')
opt_fit <- optimizing(gp_opt2, data=data, seed=95848338, hessian=FALSE)

alpha <- opt_fit$par[2]
rho <- opt_fit$par[1]
sigma <- opt_fit$par[3]

sprintf('alpha = %s', alpha)
sprintf('rho = %s', rho)
sprintf('sigma = %s', sigma)
```

Unfortunately, the extremely small marginal standard deviations
returned by the regularized maximum marginal likelihood fit
lead to very rigid functions that do not capture the structure
of the true data generating process,

```{r, comment=NA}
pred_data <- list(alpha=alpha, rho=rho, sigma=sigma, N=data$N, x=data$x, y=data$y,
                  N_predict=data$N_predict, x_predict=data$x_predict)
pred_opt_fit <- stan(file='predict_gauss.stan', data=pred_data, iter=1000, warmup=0,
                    chains=1, seed=5838298, refresh=1000, algorithm="Fixed_param")
```

```{r, comment=NA}
plot_gp_realizations(pred_opt_fit, data, true_realization,
                     "Posterior Realizations")
```

```{r, comment=NA}
plot_gp_quantiles(pred_opt_fit, data, true_realization,
                  "Posterior Quantiles")
```

```{r, comment=NA}
plot_gp_realizations(pred_opt_fit, data, true_realization,
                     "Posterior Predictive Realizations")
```

Despite the rigidity of the fitted Gaussian process, the
larger measurement variability does better capture the
data generating process than the unregularized fit.

```{r, comment=NA}
par(mfrow=c(1, 2))

plot_gp_pred_quantiles(pred_opt_fit, data, true_realization,
                       "PP Quantiles")

plot_gp_pred_quantiles(dgp_fit, data, true_realization,
                       "True DGP Quantiles")
```

Still, the overall result leaves much to be desired.

# One Singular Fluctuation

Given some of the more subtle mathematical properties of
Gaussian processes the limitations of even the regularized
maximum marginal likelihood fit are actually not all that
surprising.  In particular, Gaussian processes are notoriously
singular distributions over the Hilbert space of functions.

More intuitively, given particular kernel with particular
hyperparameters a Gaussian process does not support an entire
Hilbert space but rather only a single singular slice through
that space.  In other words, the functions realizations that
we can draw from the Gaussian process are limited to this
slice.  Changing the hyperparameters by even an infinitesimal
amount yields a different slice though the Hilbert space
that has _no overlap_ with the original slice.

Consequently, whenever we use a Gaussian process with fixed
hyperparameters we consider an infinitesimal set of functions
that could define the variate-covariate relationship.  If
the fixed hyperparameters don't exactly correspond to the
true hyperparameters -- even if they are off by the smallest
margins -- then we won't be able to generate any of the
functions that would be generated by the true data generating
process.

In practice this singular behavior isn't quite as pathological
as it sounds, as the behavior that differentiates between
neighboring slices tends to be isolated to wiggles with high
frequency and low amplitude that would have small effects.
That said, when we don't know the true values of the
hyperparameters then even these small wiggles can have
significant effects on the maximum marginal likelihood and
hence strongly affect the resulting fit.

The only way to take full advantage of a Gaussian process is
consider the entire Hilbert space of functions, and that
requires considering _all_ of the possible hyperparameters
for the chosen kernel.  Fortunately, that is a natural
consequence of the Bayesian approach which we will discuss
in [Part 3](../gp_part3/part3.html).

# Conclusion

Gaussian processes provide a flexible means of nonparametrically
modeling regression behavior, but that flexibility also
hinders the performance of point estimates derived from
maximum marginal likelihood and even regularized maximum
marginal likelihood, especially when the observed data are
sparse.  In order to take best advantage of this flexibility
we need to infer Gaussian process hyperparameters with
Bayesian methods.

[Part 3: Bayesian Inference of Gaussian Processes Hyperparameters](../gp_part3/part3.html)

# Acknowledgements

The insights motivating this case study came from a particularly
fertile research project with Dan Simpson, Rob Trangucci, and
Aki Vehtari.

I thank Dan Simpson, Aki Vehtari, and Rob Trangucci for many
helpful comments on the case study.

# Original Computing Environment

```{r, comment=NA}
writeLines(readLines(file.path(Sys.getenv("HOME"), ".R/Makevars")))
```

```{r, comment=NA}
devtools::session_info("rstan")
```
